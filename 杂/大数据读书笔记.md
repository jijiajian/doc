白话大数据与机器学习 

```
回归模型可预测连续值。例如，回归模型做出的预测可回答如下问题：

加利福尼亚州一栋房产的价值是多少？
用户点击此广告的概率是多少？
分类模型可预测离散值。例如，分类模型做出的预测可回答如下问题：

某个指定电子邮件是垃圾邮件还是非垃圾邮件？
这是一张狗、猫还是仓鼠图片？
```

## 回归

### 线性回归
#### 损失函数
平方损失：一种常见的损失函数

接下来我们要看的线性回归模型使用的是一种称为平方损失（又称为 L2 损失）的损失函数。单个样本的平方损失如下：

  = the square of the difference between the label and the prediction
  = (observation - prediction(x))2
  = (y - y')2
均方误差 (MSE) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：
### 降低损失
1. 迭代方法
2. 梯度下降法
```
    学习速率
    正如之前所述，梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

    超参数是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间：
```
3. 随机梯度下降法(SGD)


## 聚类

K-Means算法

（1）从n个向量对象任意选择k个向量作为初始聚类中心。 
（2）根据在步骤（1）中设置的k个向量（中心对象向量），计算每个对象与这k个中心对象各自的距离。 
（3）对于步骤（2）中的计算，任何一个向量与这k个向量都有一个距离，有的远有的近，把这个向量和距离它最近的中心向量对象归在一个类簇中。 
（4）重新计算每个类簇的中心对象向量位置。 
（5）重复（3）（4）两个步骤，直到类簇聚类方案中的向量归类变化极少为止。例如，一次迭代后，只有少于1%的向量还在发生类簇之间的归类漂移，那么就可以认为分类完成。

## 分类

1. 朴素贝叶斯


2. 决策树模型
信息熵  描述混乱程度 信息熵越小,混乱程度越低
信息增益  分类前的信息熵 - 分类后的信息熵
选择信息增益大的节点作为根节点


3. KNN算法